---
title: "7. Regression - Week 4 Notes"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 5, fig.align = "center")
```

# Logistic and Poisson regression
Now we're going to talk a bit about generalized linear models (GLMs). Linear models are quite useful in some scenarios, but they certainly have their limitations. Additive response models don't make that much sense if the response is discrete, or strictly positive (why?). Some transformations, such as taking a cube root, are difficult to interpret, and other interpretable transformations, like natural logs, aren't applicable to zero or negative values.

GLMs expand the notions of linear models to deal with some of these cases, although they necessarily grow in complexity as a result. A GLM involves 3 components:

* an exponential family model for the response - this is the distribution (normal is a part of the exponential family), the random component
* a systematic component via a linear predictor - this is the set of regression coefficients
* a link function that connects the means of the response to the linear predictor - this connects the random and systematic components

The 3 most famous cases of GLMs are: linear models, binomial and binary regression, and Poisson regression. We've already looked at linear models, so now we'll tackle the remaining two.

## Linear model as GLM
Let's review the linear model as our first example of GLMs. We assume that $Y_i /sim N(\mu_i, \sigma^2)$ (the Gaussian distribution is an exponential family distribution). That's the random piece.

We define the linear predictor to be $\eta_i = \sum_{k=1}^p X_{ik}\beta_{k}$.

The GLM link function is defined as $g(\mu) = \eta$, and for linear models we use the identity function $g(\mu) = \mu$ so that $\mu_i = \eta_i$.

There's an important distinction in what we've done above and how we previously treated linear models. Previously, we said that $Y$ was a function of our linear predictor and we added some random error terms. This is equivalent to saying that if we could simply measure everything about a system, then our model could perfectly predict the outcome, that is the errors would be nonexistent.

Now, we say that $Y$ is normally distributed, instead of being perfectly linear. We define $\eta$ as the linear *component* of our model, and then we tie the randomness of $Y$ to the linearity of $\eta$ with a link function. This is a different kind of compartmentalization of the model.

### Logistic regression
Now let's think about coin flips. Modeling these as normal doesn't make much sense. Instead, let's assume that $Y_i /sim Bernoulli(\mu_i)$ so that $E[Y_i] = \mu_i$ where $0 \le \mu_i \le 1$.

Our linear predictor is still $\eta_i = \sum_{k=1}^p X_{ik}\beta_k$, same as always.

But our link function is now different. It's the natural log of the odds, referred to as the logit and it's $g(\mu) = \eta = \log \left( \frac {\mu} {1 - \mu} \right)$.

So we can write out the binomial likelihood like this (although I have no idea what this means yet)

$$\prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1- y_i} = \exp \left(\sum_{i=1}^n y_i \eta_i \right) \prod_{i=1}^n (1 + \eta_i)^{-1}$$

and so that is the function that we want to optimize (maximize the likelihood) over to obtain our parameter estimates.

### Poisson regression
We can also use this GLM technique for Poisson random variables. Assume that $Y_i /sim Poisson(\mu_i)$ so that $E[Y_i] = \mu_i$ where $0 \le \mu_i$.

The linear predictor is just $\eta_i = \sum_{k=1}^p X_{ik}\beta_k$, as always.

The link function is just the log $g(\mu) = \eta = \log(\mu)$. And then we can maximize the likelihood.

## GLM notes
In each case, the only way in which the likelihood depends on the data is through

$$\sum_{i=1}^n y_i \eta_i = \sum_{i=1}^n y_i \sum_{k=1}^p X_{ik} \beta_k = \sum_{k=1}^p \beta_k \sum_{i=1}^n X_{ik} y_i$$

This is the point of the GLM method, to abstract the model into these generalized components that we can work with in a standard way. The prime example is the equation for the maximum likelihood, which is

$$0 = \sum_{i=1}^n \frac {Y_i - \mu_i} {Var(Y_i)} W_i$$

This equation looks a lot like a least squares sort of equation, only there's a set of weights and a variance in the denominator that doesn't go away.

Regarding variances, remember that when we previously treated linear models, we assumed $Var(Y_i) = \sigma^2$, a constant. The assumptions about variance change for Bernoulli and Poisson models.

For Bernoulli, $Var(Y_i) = \mu_i (1 - \mu_i)$.

For Poisson, $Var(Y_i) = \mu_i$.

You can check your own data to see if it meets these assumptions, but if it doesn't, there are "quasi" distribution functions that can be used to relax the assumptions of the variance model.

## Logistic regression
Now let's dig into the meat of the 2 most important cases in GLMs, the first being logistic regression. Here we're talking about Bernoulli variables, 0s and 1s, binary outcomes. This could be alive/dead, win/loss, success/failure, whatever. The special case of *binary logistic regression* relates to situations where our covariant is constant, that is the probability of success is constant. In generalized logistic regression, we consider the case when the probability of success is some function of covariants (not constant).

We're going to use the following data for our example. It's some data regarding Ravens points and wins.

```{r download, eval=TRUE, echo=FALSE}
## setup URL and path variables for this data
ravensurl <- "https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"
datapath <- "./data"
ravenspath <- paste0(datapath, "/ravensData.rda")

## create the data directory if necessary
if(!dir.exists(datapath)) {
     dir.create(datapath)
     print("Created /data directory")
} else {
     print("/data directory already exists")
}

## If there's no data, and no .zip, then download and unzip data
## If there's no data, but there is a .zip, then unzip data
if(!file.exists(ravenspath)) {
  download.file(ravensurl, destfile = ravenspath)
  print("Downloaded & extracted ravens data file")
} else {
  print("Found ravens data, ready to go")
}

load(ravenspath)
```

We probably don't want to fit a linear model to this data; it doesn't really make sense as the outcome isn't continuous, it's binary. A linear model might look like this

$$RW_i = b_0 + b_1 RS_i + e_i$$

* $RW_i$ is 1 or 0, win or loss
* $RS_i$ is number of points for Ravens
* $b_0$ probability of Ravens win if they score 0 points
* $b_1$ increase in probability of a win for each Ravens point
* $e_i$ residual variation

It's probably much better to model the odds. Remember that the odds are defined as $\frac{p}{1 - p}$. (Conversely, the probability $p = \frac{odds}{1 - odds}$.) These are the definitions of the quantities we're working with:

Binary Outcome 0/1

$$RW_i$$

Probability (0, 1)

$$ Pr(RW_i | RS_i, b_0, b_1)$$

Odds $(0, \inf)$

$$\frac {Pr(RW_i | RS_i, b_0, b_1)} {1 - Pr(RW_i | RS_i, b_0, b_1)}$$

Log odds $(-\inf, \inf)$

$$\log \left( \frac {Pr(RW_i | RS_i, b_0, b_1)} {1 - Pr(RW_i | RS_i, b_0, b_1)} \right)$$

So here's a little bit of the mathematical brachiation that takes us from linear regression to logistic regression. Under linear regression, our model would be $RW_i = b_0 + b_1 RS_i + e_i$, which is the same thing as calling $RW_i$ the expected value of $RW_i$, making $E[RW_i | RS_i, b_0, b_1] = b_0 + b_1 RS_i$.

Now, for a Bernoulli variable, the expected value is just the probability of the outcome, so we could model the probability of that outcome with something like

$$Pr(RW_i | RS_i, b_0, b_1) = \frac {\exp(b_0 + b_1 RS_i)} {1 + \exp(b_0 + b_1 RS_i)}$$

so the probability is equal to $e$ to the linear regression over $1 + e$ to the linear regression. Manipulating that term on the right yields

$$\log \left( \frac {Pr(RW_i | RS_i, b_0, b_1)} {1 - Pr(RW_i | RS_i, b_0, b_1)} \right) = b_0 + b_1 RS_i$$

So the *log of the odds* is the linear regression relationship. That *log of the odds* term is known as the *logit*, and you can flip between the odds and the probability with the logit and its counterpart, the expit $\frac {e^a} {1 + e^a}$, where $a = b_0 + b_1 RS_i$.

What does all this mean? It's means that in logistic regression, we're modeling every trial as a coin flip where the success probability changes with $i$, and we're relating that probability to the regressors via the log of the odds.

### Interpreting logistic regression
Interpreting $b_0 + b_1RS_i$ is pretty straightforward. Plug in $RS_i = 0$ and you have $b_0$, which is the ~~probability~~ log odds of a Ravens win (remember, we're dealing in success probabilities) if their score $RS = 0$. This is kind of like the intercept term. So $\frac {e^{b_0}} {1 + e^{b_0}}$ is the *probability* associated with the log odds.

$b_1$ is the slope term. $b_1 = b_0 + b_1(RS_i + 1) - [b_0 + b_1(RS_i)]$, which is the difference in log odds between scoring this point and *one more* point (change per unit).

### History of odds
Imagine you're playing a game where you flip a coin with success probability $p$. If heads, you win $X$, if tails, you lose $Y$. What $X$ and $Y$ make the game fair?

$$E[earnings] = Xp - Y(1 - p) = 0$$

which implies that

$$\frac {Y}{X} = \frac {p}{1 - p}$$

$Y$ represents how much you should be willing to put up (and thus be willing to lose) to gain $X$ dollars.

### Visualizing logistic regression
So let's take a look at what this actually looks like. Remember, our `beta0` and `beta1` terms are like the intercept/slope terms.

```{r}
library(ggplot2)
x <- seq(-10, 10, length = 1000)
beta0 <- 0
beta1 <- 2
logit <- function(x, beta0, beta1) {
     qplot(x = x, y = exp(beta0 + beta1*x) / (1 + exp(beta0 + beta1*x)))
}
logit(x, beta0, beta1)
```

We can see that the `beta0` term moves the entire function left/right on the x-axis (the "intercept").

```{r}
logit(x, 5, 2)
```

And the `beta1` term manipulates the "slope" of the logistical threshold.

```{r}
## gradual threshold
logit(x, 0, .5)
```

```{r}
## steep threshold
logit(x, 0, 5)
```

Also, we can see the effect of flipping the sign of this `beta1` term, which is to flip around the y-axis.

```{r}
## negative beta1
logit(x, 0, -2)
```

### More logistic regression
Let's use the Ravens win data to fit a model via the `glm` function.

```{r}
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore, family = "binomial")
summary(logRegRavens)
```

So now we can interpret these results. Let's make a quick plot of the scores from our original data and the fitted values from our `glm` model.

```{r}
qplot(ravensData$ravenScore, logRegRavens$fitted.values)
```

Remember what those fitted values are: they're the score multiplied by the score coefficient, add the intercept term, then use the expit to convert to the actual probability scale. Notice that this plot only contains our original data's scores, so the logistic curve is incomplete.

If we exponentiate the coefficients we would get:
```{r}
exp(logRegRavens$coefficients)
```

## Poisson GLM
This all refers to unbounded count data, like calls to a call center, number of flu cases, number of cars that cross a bridge. Data can frequently be in the form of rates, like cars per unit time, % of kids passing a test. Linear regression with transformation is an option here...

The Poisson distribution is a useful model for counts and rates. They work well for things like web traffic hits, incidence rates, approximating binomial probabilities with small $p$ and large $n$ (success probability is small with large number of trials). This also works for contingency tables. So you sample people from a population, count black/brown/blonde hair, then cross tabulate that with counts of brown/blue/green eyes.

### The Poisson mass function

* $X ~ Poisson(t\lambda)$ if

$$P(X=x) = \frac{(t\lambda)^x e^{-t\lambda}} {x!}$$

for $x = 0, 1, ...$

* The mean of the Poisson distribution is $E[X] = t\lambda$, therefore $E[X/t] = \lambda$
* the variance of the Poisson distribution is $Var(X) = t\lambda$
* the Poisson tends to a normal distribution as $t\lambda$ gets large

We'll start by taking a look at the website traffic data for J.Leek's website.

```{r, eval=TRUE, echo=FALSE}
## setup URL and path variables for this data
trafficurl <- "https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda"
datapath <- "./data"
trafficpath <- paste0(datapath, "/gaData.rda")

## create the data directory if necessary
if(!dir.exists(datapath)) {
     dir.create(datapath)
     print("Created /data directory")
} else {
     print("/data directory already exists")
}

## If there's no data, and no .zip, then download and unzip data
## If there's no data, but there is a .zip, then unzip data
if(!file.exists(trafficpath)) {
  download.file(trafficurl, destfile = trafficpath)
  print("Downloaded & extracted ravens data file")
} else {
  print("Found ravens data, ready to go")
}

load(trafficpath)
```

Futz with the date column a bit, then plot visits over time. Even though we know we can't exactly use a linear model for this data, let's go ahead and use it to throw a trendline on here.

```{r}
gaData$julian <- julian(gaData$date)
lml <- lm(gaData$visits ~ gaData$julian)
ggplot(data = gaData, mapping = aes(x = julian, y = visits)) + geom_point() + geom_smooth(method = "lm", color = "red", size = 2)
```

So since linear models don't really make great sense with Poisson data, we might start by taking the natural log of the outcome. So the model would be:

$$log(NH_i) = b_0 + b_1JD_i + e_i$$

where
* $NH_i$ is the number of hits on the $i^{th}$ instant
* $JD_i$ is the $i^{th}$ julian day
* $b_0$ is the log number of hits on Julian day 0 (intercept)
* $b_1$ is the increase in log number of hits per unit day (slope)
* $e_i$ is the variation due to everything we didn't measure (error)

Now we discuss expenentiating coefficients (to make them interpretable).

$e^{E[log(Y)]}$ is the population geometric mean of $Y$. Here's why we call it that:

The *empirical* geometric mean *of a sample* is $(\Pi_{i=1}^n y_i)^{1/n}$, the product of a sample, raised to $1/n$. If we take the log of that, we get the arithmetic mean, the ordinary mean (thus, the expected value), of the log data, which is $E[log(Y)] = \frac{1}{n} \Sigma_{i=1}^n log(y_i)$.

So the geometric mean is just exponentiating the arithmetic mean of the log data.

We know that with lots of data, that arithmetic mean will converge to something, and that something is the geometric mean (of the population, as $n$ becomes large).

$$e^{\frac{1}{n} \Sigma_{i=1}^{n} log(y_i)} = (\Pi_{i=1}^{n} y_i)^{1/n}$$

This all means that when you take the natural log of outcomes and fit a regression model, your *exponentiated* coefficients are interpretable with respect to geometric means.

$e^{\beta_0}$ estimates the geometric mean hits on day 0 (of course, you should ensure that "day 0" is meaningful, unlike in this example)

$e^{\beta_1}$ estimates relative increase/decrease in geometric mean hits per day

Note that if you have 0s in your data, then `log` won't work, so it's not unusual to add 1 to your data to avoid this problem.

```{r}
round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 4)
```

So the model is predicting 0 mean hits on day 0 (again, useless in this example), and a 0.2% increase per day.

### Linear vs. Poisson regression

#### Linear

$$ NH_i = b_0 + b_1JD_i + e_i$$

or

$$E[NH_i|JD_i, b_0, b_1] = b_0 + b_1JD_i$$

#### Poisson/log-linear

$$log(E[NH_i|JD_i, b_0, b_1]) = b_0 + b_1JD_i$$

or

$$E[NH_i|JD_i, b_0, b_1] = exp(b_0 + b_1JD_i)$$

Now we can plot the linear vs. the Poisson regression to show that Poisson gives us some curvature, and also makes more sense than a linear model for count data.

```{r}
ggplot(data = gaData, mapping = aes(x = julian, y = visits)) + geom_point() + geom_smooth(method = "lm", se = FALSE, color = "red") + geom_smooth(method = "glm", se = F, method.args = list(family = "poisson"), color = "blue")
```

## Hodgepodge
