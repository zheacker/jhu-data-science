---
title: "8. Practical ML - Week 1 Notes"
output: html_notebook
---

# Motivation & Pre-requisites
This course covers the basic ideas behind machine learning/prediction.

- study design - training vs. test sets
- conceptual issues - out of sample error, ROC curves
- practical implementations - the `caret` package

## Who predicts things?
Local governments predict pension payments. Google predicts your ad clicks. Amazon/Netflix predict what movies you'll watch. Insurance companies predict your risk profile.

## `caret`
This package is awesome, we'll dig into it some more in this course.

# What is prediction?
Using characteristics about some data points to determine the relationship between those characteristics and the class of those data points. You use probability/sampling to build test/training sets, then you develop a prediction algorithm.

## Steps in prediction
You start with a question (of course) and your input data. You either use existing features, or you can develop them computationally, and then use some ML algorithm to predict stuff. You hone the parameters of that algorithm, then you transfer it over to a test dataset in order to evaluate its performance.

## the SPAM example
He goes through the SPAM data from the `kernlab` package. Use word frequency of the word "your" to build a threshold qualifier, then evaluate its efficacy (but only on the original dataset, which is kind of worthless).

# Relative importance of steps
So the relative importance of the steps for this ML process are: question > data > features > algorithms. **The most important step is the question.** Can't stress this enough, you have to know what you're shooting for. Then the data is the next most important component. Then making sure you get your features right. The algorithm is the least important (usually). Lots of algorithms can typically perform equally well, so don't get too hung up here.

Remember, garbage in => garbage out. Predicting new movie ratings based on old movie ratings would probably be pretty easy, because it's the same type of data. It would be harder to predict disease from just gene expression data; even though they're related, mapping their relationship can be tough.

What makes for good features? Well, most importantly, they capture the fundamental relationship that you're trying to leverage. There needs to be a real-world relationship between the data you put into an algorithm and the prediction it's trying to make. Good features also compress the data while retaining the most useful information. They should be created based on expert knowledge of the subject.

Some common mistakes:
Trying to automate feature selection. This is usually a bad idea. Black boxes can work, but it's almost always better to know what's going on in there. Also be sure to pay attention to the specifics of your dataset (know thy data!) and avoid throwing away data unnecessarily.

### Qualities of good ML methods
If you're showing it to somebody/need to explain it, it needs to be interpretable. PCA can compress data and still produce great features, but it might be impossible to know what they mean.

It needs to be simple. Similar to interpretable.

Simplicity/interpretability might be a tradeoff for accuracy. You want an accurate method, but you must balance that with the other considerations.

You also want it to be fast and scalable. Particularly in business, these things need to be built with systems in mind.

# In-sample & out-of-sample errors
In-sample is the error rate you get on the same data set that you used to build your predictor. Sometimes called resubstitution error. This will always be a little optimistic because it is pretty much guaranteed to overfit just a little bit to the training data set.

The out-of-sample error is the error rate you get on a new data set. Sometime called the generalization error. This is what you really care about, because the point of your algorithm is to go into new data and predict stuff.

In-sample < out-of-sample error (always). Again, this is due to overfitting on the training set.

# Prediction study design

1. define your error rate
2. split data into
     - training, testing, validation (optional)
3. on the training set, pick features
     - use cross-validation
4. on the training set, pick the prediction function
     - use cross-validation
5. if no validation
     - apply to test set **exactly 1 time**
6. if validation set
     - apply to test set and refine
     - apply to validation set **exactly 1 time**
     
The point is that there should always be 1 data set that you leave out and subject to your model **exactly once**. This allows you to measure your model's performance on completely new data.

The Netflix competition used the training > probe > quiz > test datasets methodology. Basically, at every step along that progression, you get to tune your model a bit more. But you only get to test your model on the *last* dataset one time.

The problem with this is that you need to try to avoid small sample sizes. Obviously, if your dataset is really small, you introduce the likelihood that your algorithm is landing correct predictions *by chance*, which is of course not what you want. General rules of thumb for dataset sizes:

- given a large sample size
  - 60% training
  - 20% test
  - 20% validation
- given medium sample size
  - 60% training
  - 40% test
- given small sample size
  - do cross validation
  - report caveat of small sample size
  - but really, get more data

So remember, set aside your final test dataset and *don't look at it*. In general, you want to randomly sample the data to build the train/test sets. Some care must be taken to reflect the structure of the problem, such as time series data or other dependencies within the data (called backtesting in finance), but you want the most diversity possible in all datasets.

# Types of errors
So let's call items we identify "positive" and items we reject "negative." That means that:

* true positive = correctly identified
* false positive = incorrectly identified
* true negative = correctly rejected
* false negative = incorrectly rejected

Here's a medical testing example:

* true positive = sick people correctly diagnosed as sick
* false positive = well people incorrectly diagnosed as sick
* true negative = well people correctly diagnosed as well
* false negative = sick people incorrectly diagnosed as well

From these definitions we can derive the following metrics of model performance:

* sensitivity: Pr(positive test | disease); $\frac {TP}{TP + FN}$
* specificity: Pr(negative test | no disease); $\frac {TN}{FP + TN}$
* \+ predictive value: Pr(disease | positive test); $\frac {TP}{TP + FP}$
* \- predictive value: Pr(no disease | negative test); $\frac {TN}{FN + TN}$
* accuracy: Pr(correct prediction); $\frac {TP + TN} {TP + FP + FN + TN}$

The above metrics work well for binary/classification errors, but you need something different for continuous data. Unlike in classification problems, with continuous data you're also concerned with *how close* you got to the right answer. For this, we might use the mean squared error (MSE) or the root mean squared error (RMSE):

Mean squared error:

$$ \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2 $$

Root mean squared error:

$$ \sqrt{ \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2 } $$

where $\hat y_i$ is our prediction and $y_i$ is the true value for the $i^{th}$ element. The MSE takes the difference between the two (our error) and squares them to force them positive, then averages that error over all data points.

The RMSE is the same thing, but taking the square root allows us to interpret the error on the same scale as the original data, as opposed to the squared scale.

