---
title: "5. Reproducible Research - Week 1 Notes"
output: html_notebook
---

# Concepts, Ideas, Structure
Reproducibility is important because it's basically the 'score' of data analysis. It's the blueprint to recreate your analysis, just like a symphony uses a score to reproduce a performance. It's not about a performance, it's not about executing the piece of music once. The goal is to develop a set of instructions that any musician can follow to reproduce this work.

In data analysis, we don't really have a 'language' that we've landed on to communicate these instructions. We'll focus on R markdown here, which is a good method for analyses done in R.

#### Replication
In science, replication is incredibly important. If something is true, we should be able to replicate it. (Gravity works every time.) This is one reason why reproducible research is important. Replication is hard and expensive; it's the gold standard for scientific inquiry, but it's not always feasible. Reproducible research can help bridge the gap to replication.

Reproducible research can help others understand and repeat your analysis, and they'll be able to more quickly get insights into what you did/how you did it/whether you fucked up.

## So what do we need?
What do we need to make reproducible research?

- analytic data is available
- analytic code is available
- documentation of code and data
- standard means of distribution

## Enter Literate (Statistical) Programming
The idea here is that an article/analysis is a stream of text and code. Analysis code is broken up into text and code "chunks". Each code chunk loads data and computes some results. Presentation code formats results and the article text just explains what's going on. Literate programs can be **weaved** to produce human readable documents and **tangled** to produce machine readable documents.

One of the first tools for this in R was Sweave, which used LaTeX and R as its documentation and programming languages, respectively. It's kinda limited and not really active anymore.

Now there's **knitr**! Knitr uses R for programming, but can use other documentation languages (LaTeX, markdown, HTML). Knitr is cool.

## Steps in Data Analysis

### 1. Define the question
This is a critical step. Defining the question is the most important dimensionality reduction tool that you have. In general, the science will determine the kind of question that you ask. That will point you to some data. You will apply some statistical methods to that data to learn something, and then, if you're ambitious, you might develop some theoretical statistics to generalize your findings.

So you might start with a general question: Can I automatically detect spam emails? This needs to be transformed into a more concrete question for data science: can I use characteristics of the emails themselves to classify them as spam/not spam?

### 2. Define the ideal dataset
Once you have a question, the dataset may depend on your goal:

- descriptive: use the whole population
- exploratory: use a random sample with many variables measured
- inferential: use the *right* population, randomly sampled
- predictive: use a training and test dataset from the same opulation
- causal: use data from a randomized study
- mechanistic: use data about the entirety of the system

### 3. Define what data you can access
Sometimes data is free on the web. Sometimes you have to buy data. Always respect the terms of use. If the data doesn't exist, you may need to generate the data yourself.

In the spam example, you likely can't get access to gmail's server farms, and you'd be arrested if you tried, so you can find a free spam classifier dataset on the UCI Machine Learning Repository.

### 4. Obtain the data
When you obtain data, make sure to get the rawest form that you can get. Reference the source, including the URL, and the date/time you accessed the data. And maybe host your own copy?

### 5. Clean the data
The raw data now needs to be processed. You need to make sure you understand any processing that happened before you got your hands on the data, and then take it from there. The processing may include reformatting the data, sampling, tidying, etc. Then, and this is crucial, you need to make a determination as to the quality of the data and its ability to answer your question. If it's not good enough, then either quit or get new data.

This data comes from the `kernlab` package, so let's load it up and get started.

```{r spam}
library(kernlab)
data(spam)
str(spam[, 1:15])

## and then we can split it into test/train data
set.seed(3435)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
table(trainIndicator)

trainSpam = spam[trainIndicator == 1, ]
testSpam = spam[trainIndicator == 0, ]
```

### 6. Exploratory data analysis
To start, we want to look at summaries of the data, check for any missing data, create some exploratory plots, etc.

The spam dataset is a basically a bunch of words and their respective frequencies. Below, we'll investigate it a bit.

```{r recon}
## Just have a look at the data
names(trainSpam)
head(trainSpam)
table(trainSpam$type)

## Now we'll plot the avg # of capital letters by class, as boxplots
plot(trainSpam$capitalAve ~ trainSpam$type)

## but this is tough to see, so let's take the log and add 1
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)

## now we plot relationships between the 1st 4 variables (again, log + 1)
plot(log10(trainSpam[, 1:4] + 1))

## and now we make a tree (dendogram?)
hCluster = hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)

## again, log + 1 to clear it up
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
plot(hClusterUpdated)
```

### 7. Statistical prediction/modeling
Now we can try to do some real statistcal modeling. This should be informed by your exploratory analysis, and the exact method you use could depend on the question of interest and what your overarching goal is. You should be careful to account for any transformations/processing that the data has undergone, and it's always important to report your measures of uncertainty.

Below, we'll loop over every variable in the dataset and develop a model based solely on that variable. Then, we'll check for the most accurate of those models.

```{r models, warning=FALSE}
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x, y) sum(x != (y > 0.5))
cvError = rep(NA, 55)
library(boot)
for(i in 1:55) {
     lmFormula = reformulate(names(trainSpam)[i], response = "numType")
     glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
     cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}

names(trainSpam)[which.min(cvError)]
```

Turns out `charDollar` (the number of `$` characters in an email) has the lowest cross-validated error, so we'll use the model built on that variable.

```{r predictions, warning=FALSE}
## refit that model
predictionModel = glm(numType ~ charDollar, family = "binomial", data = trainSpam)

## get predictions on the test set
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep("nonspam", dim(testSpam)[1])

## classify as spam for those with probability > 0.5
predictedSpam[predictionModel$fitted > 0.5] = "spam"

## build a classification table
table(predictedSpam, testSpam$type)
```

### 8. Interpret results
It's important to use the right language when interpreting your results; don't claim more than you've discovered. If you can give a bit of an explanation as to why you got these results, give it. Try to interpret coefficients and your measures of uncertainty.

For this example, explain that the fraction of characters that are `$` can be used to predict spam emails (also note cases where this wouldn't work, such as maybe in the finance sector). Explain the overall model: here, more `$` always means more likely spam, but that could be different. Give the test set error rate of 22.4%.

### 9. Challenge results
Now it's important to challenge those results (which I touched on up there). You've got to think through every step of the analysis: the question, the data source, the processing, the analysis, and the conclusions. Challenge the measure of uncertainty, and think of any potential alternative analytical methods.

### 10. Synthesize/write up results
Now you write it all up. Lead with the question, summarize the analysis into the story. But you don't have to include absolutely every analysis into the final product. Only include points that are a) needed for the story and b) needed to address a challenge. That being said, keeping some notes on *every* analysis can be helpful if you'll be responding to questions.

Also, you should order the analysis according to the story, rather than adhering to chronology. Include "pretty" figures that contribute to the story.

For this example, the question was "can I predict spam based on quantitative characteristics of the emails?" The approach was to collect the data from UCI, then make training/test sets. We explored some relationships, chose a logistic model based on cross validation error in the training set, then applied it to the test set for 78% accuracy. The conclusion was that dollar sign characters indicate spam, which makes sense, e.g. "Make money with viagra$$$$"

However, 78% isn't great, we could use more variables to build a more complex model (and hopefully more predictive). And we could base a model on something other than logistic regression.

### 11. Create reproducible code
Then you need to make reproducible code with R markdown and knitr.

## Organizing your analysis
What are the main types of file that you'll have at the end of an analysis?

1. data
     - raw data: back it up, host it yourself, maybe store in git (if small enough), and record access info in README
     - processed data: name it according to the processing script, should be tidy
2. figures
     - exploratory stuff: name according to scripts, these aren't vital, but worth keeping up with
     - polished stuff: very important, make reproducible, maybe backup
3. R code
     - raw/unused scripts: not the final product, but still keep track of them! name them well, comment them well
     - final scripts: for the final paper/report/product, extremely well commented
     - R markdown files: this is probably the best way to keep everything together
4. text
     - README files: I'll use these mainly for git, R markdown takes over this role
     - polished/formal report: I'm not quite doing this yet